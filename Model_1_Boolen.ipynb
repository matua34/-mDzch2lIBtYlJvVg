{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>0</td>\n",
       "      <td>2143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>1506</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  age           job  marital  education  default  balance  \\\n",
       "0           0   58    management  married   tertiary        0     2143   \n",
       "1           1   44    technician   single  secondary        0       29   \n",
       "2           2   33  entrepreneur  married  secondary        0        2   \n",
       "3           3   47   blue-collar  married    unknown        0     1506   \n",
       "4           4   33       unknown   single    unknown        0        1   \n",
       "\n",
       "   housing  loan  contact  day month  duration  campaign  y  \n",
       "0        1     0  unknown    5   may       261         1  0  \n",
       "1        1     0  unknown    5   may       151         1  0  \n",
       "2        1     1  unknown    5   may        76         1  0  \n",
       "3        1     0  unknown    5   may        92         1  0  \n",
       "4        0     0  unknown    5   may       198         1  0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "data = pd.read_excel(\"DATA_1.xlsx\")\n",
    "datayedek = data.copy() #datamın yedeğini de aldım.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buradan sonra kategorik değişkenleri modol de kullanmayacağım için data da atıcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"job\",\"marital\",\"education\",\"contact\",\"month\"],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Unnamed: 0\",axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1506</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  default  balance  housing  loan  day  duration  campaign  y\n",
       "0   58        0     2143        1     0    5       261         1  0\n",
       "1   44        0       29        1     0    5       151         1  0\n",
       "2   33        0        2        1     1    5        76         1  0\n",
       "3   47        0     1506        1     0    5        92         1  0\n",
       "4   33        0        1        0     0    5       198         1  0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() #şimdi hazırız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =data[\"y\"].values\n",
    "x = data.drop(\"y\",axis =1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test =train_test_split(x,y,test_size =0.33,random_state=15)\n",
    "#rondom_state önemli değildir. eğer bir başkası bu modeli 15 ile oluşturursa aynı değerler dönürür.\n",
    "#genel olarak %30 gibi bir değeri test için ayırırz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26800, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13200,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units =8,activation=\"relu\"))\n",
    "model.add(Dense(units =5,activation=\"relu\"))\n",
    "model.add(Dense(units =5,activation=\"relu\"))\n",
    "model.add(Dense(units =5,activation=\"relu\"))\n",
    "model.add(Dense(units =5,activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1,activation=\"sigmoid\")) #diğer modelde ise msekullanacağım.\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer =\"adam\") #diğer modelde ise rmsprop kullanacağım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\metea\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 26800 samples, validate on 13200 samples\n",
      "Epoch 1/300\n",
      "26800/26800 [==============================] - 4s 153us/sample - loss: 0.2944 - val_loss: 0.2354\n",
      "Epoch 2/300\n",
      "26800/26800 [==============================] - 5s 172us/sample - loss: 0.1960 - val_loss: 0.1831\n",
      "Epoch 3/300\n",
      "26800/26800 [==============================] - 4s 151us/sample - loss: 0.1808 - val_loss: 0.1806\n",
      "Epoch 4/300\n",
      "26800/26800 [==============================] - 2s 66us/sample - loss: 0.1801 - val_loss: 0.1821\n",
      "Epoch 5/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1795 - val_loss: 0.1774\n",
      "Epoch 6/300\n",
      "26800/26800 [==============================] - 3s 130us/sample - loss: 0.1792 - val_loss: 0.1790\n",
      "Epoch 7/300\n",
      "26800/26800 [==============================] - 4s 138us/sample - loss: 0.1787 - val_loss: 0.1765\n",
      "Epoch 8/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1788 - val_loss: 0.1768\n",
      "Epoch 9/300\n",
      "26800/26800 [==============================] - 4s 144us/sample - loss: 0.1787 - val_loss: 0.1772\n",
      "Epoch 10/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1777 - val_loss: 0.1763\n",
      "Epoch 11/300\n",
      "26800/26800 [==============================] - 4s 138us/sample - loss: 0.1780 - val_loss: 0.1759\n",
      "Epoch 12/300\n",
      "26800/26800 [==============================] - 4s 140us/sample - loss: 0.1778 - val_loss: 0.1777\n",
      "Epoch 13/300\n",
      "26800/26800 [==============================] - 4s 143us/sample - loss: 0.1777 - val_loss: 0.1775\n",
      "Epoch 14/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1776 - val_loss: 0.1776\n",
      "Epoch 15/300\n",
      "26800/26800 [==============================] - 4s 138us/sample - loss: 0.1773 - val_loss: 0.1753\n",
      "Epoch 16/300\n",
      "26800/26800 [==============================] - 4s 137us/sample - loss: 0.1772 - val_loss: 0.1749\n",
      "Epoch 17/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1770 - val_loss: 0.1763\n",
      "Epoch 18/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1775 - val_loss: 0.1757\n",
      "Epoch 19/300\n",
      "26800/26800 [==============================] - 4s 143us/sample - loss: 0.1772 - val_loss: 0.1773\n",
      "Epoch 20/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1768 - val_loss: 0.1746\n",
      "Epoch 21/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1764 - val_loss: 0.1753\n",
      "Epoch 22/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1766 - val_loss: 0.1745\n",
      "Epoch 23/300\n",
      "26800/26800 [==============================] - 1s 52us/sample - loss: 0.1764 - val_loss: 0.1755\n",
      "Epoch 24/300\n",
      "26800/26800 [==============================] - 1s 50us/sample - loss: 0.1763 - val_loss: 0.1762\n",
      "Epoch 25/300\n",
      "26800/26800 [==============================] - 3s 114us/sample - loss: 0.1768 - val_loss: 0.1746\n",
      "Epoch 26/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1765 - val_loss: 0.1744\n",
      "Epoch 27/300\n",
      "26800/26800 [==============================] - 4s 145us/sample - loss: 0.1761 - val_loss: 0.1744\n",
      "Epoch 28/300\n",
      "26800/26800 [==============================] - 4s 142us/sample - loss: 0.1763 - val_loss: 0.1745\n",
      "Epoch 29/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1760 - val_loss: 0.1766\n",
      "Epoch 30/300\n",
      "26800/26800 [==============================] - 4s 143us/sample - loss: 0.1760 - val_loss: 0.1753\n",
      "Epoch 31/300\n",
      "26800/26800 [==============================] - 4s 140us/sample - loss: 0.1762 - val_loss: 0.1742\n",
      "Epoch 32/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1763 - val_loss: 0.1767\n",
      "Epoch 33/300\n",
      "26800/26800 [==============================] - 3s 116us/sample - loss: 0.1759 - val_loss: 0.1741\n",
      "Epoch 34/300\n",
      "26800/26800 [==============================] - 3s 104us/sample - loss: 0.1761 - val_loss: 0.1741\n",
      "Epoch 35/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1756 - val_loss: 0.1737\n",
      "Epoch 36/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1757 - val_loss: 0.1733\n",
      "Epoch 37/300\n",
      "26800/26800 [==============================] - 4s 142us/sample - loss: 0.1759 - val_loss: 0.1735\n",
      "Epoch 38/300\n",
      "26800/26800 [==============================] - 4s 148us/sample - loss: 0.1757 - val_loss: 0.1740\n",
      "Epoch 39/300\n",
      "26800/26800 [==============================] - 3s 116us/sample - loss: 0.1754 - val_loss: 0.1732\n",
      "Epoch 40/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1754 - val_loss: 0.1750\n",
      "Epoch 41/300\n",
      "26800/26800 [==============================] - 4s 145us/sample - loss: 0.1753 - val_loss: 0.1736\n",
      "Epoch 42/300\n",
      "26800/26800 [==============================] - 3s 112us/sample - loss: 0.1754 - val_loss: 0.1735\n",
      "Epoch 43/300\n",
      "26800/26800 [==============================] - 4s 133us/sample - loss: 0.1749 - val_loss: 0.1730\n",
      "Epoch 44/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1747 - val_loss: 0.1731\n",
      "Epoch 45/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1750 - val_loss: 0.1733\n",
      "Epoch 46/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1747 - val_loss: 0.1753\n",
      "Epoch 47/300\n",
      "26800/26800 [==============================] - 3s 100us/sample - loss: 0.1747 - val_loss: 0.1751\n",
      "Epoch 48/300\n",
      "26800/26800 [==============================] - 4s 143us/sample - loss: 0.1747 - val_loss: 0.1741\n",
      "Epoch 49/300\n",
      "26800/26800 [==============================] - 4s 141us/sample - loss: 0.1743 - val_loss: 0.1740\n",
      "Epoch 50/300\n",
      "26800/26800 [==============================] - 4s 144us/sample - loss: 0.1747 - val_loss: 0.1729\n",
      "Epoch 51/300\n",
      "26800/26800 [==============================] - 3s 98us/sample - loss: 0.1745 - val_loss: 0.1753\n",
      "Epoch 52/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1745 - val_loss: 0.1745 0.17\n",
      "Epoch 53/300\n",
      "26800/26800 [==============================] - 3s 112us/sample - loss: 0.1745 - val_loss: 0.1727\n",
      "Epoch 54/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1744 - val_loss: 0.1754\n",
      "Epoch 55/300\n",
      "26800/26800 [==============================] - 4s 136us/sample - loss: 0.1742 - val_loss: 0.1736\n",
      "Epoch 56/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1741 - val_loss: 0.1723\n",
      "Epoch 57/300\n",
      "26800/26800 [==============================] - 2s 79us/sample - loss: 0.1741 - val_loss: 0.1728\n",
      "Epoch 58/300\n",
      "26800/26800 [==============================] - 3s 128us/sample - loss: 0.1747 - val_loss: 0.1723\n",
      "Epoch 59/300\n",
      "26800/26800 [==============================] - 1s 48us/sample - loss: 0.1742 - val_loss: 0.1743\n",
      "Epoch 60/300\n",
      "26800/26800 [==============================] - 4s 134us/sample - loss: 0.1745 - val_loss: 0.1772\n",
      "Epoch 61/300\n",
      "26800/26800 [==============================] - 3s 103us/sample - loss: 0.1738 - val_loss: 0.1722\n",
      "Epoch 62/300\n",
      "26800/26800 [==============================] - 3s 104us/sample - loss: 0.1739 - val_loss: 0.1722\n",
      "Epoch 63/300\n",
      "26800/26800 [==============================] - 3s 95us/sample - loss: 0.1739 - val_loss: 0.1764\n",
      "Epoch 64/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1742 - val_loss: 0.1729\n",
      "Epoch 65/300\n",
      "26800/26800 [==============================] - 4s 140us/sample - loss: 0.1739 - val_loss: 0.1728\n",
      "Epoch 66/300\n",
      "26800/26800 [==============================] - 4s 146us/sample - loss: 0.1740 - val_loss: 0.1721\n",
      "Epoch 67/300\n",
      "26800/26800 [==============================] - 3s 128us/sample - loss: 0.1738 - val_loss: 0.1731\n",
      "Epoch 68/300\n",
      "26800/26800 [==============================] - 3s 127us/sample - loss: 0.1740 - val_loss: 0.1730\n",
      "Epoch 69/300\n",
      "26800/26800 [==============================] - 4s 131us/sample - loss: 0.1737 - val_loss: 0.1722\n",
      "Epoch 70/300\n",
      "26800/26800 [==============================] - 3s 127us/sample - loss: 0.1737 - val_loss: 0.1732\n",
      "Epoch 71/300\n",
      "26800/26800 [==============================] - 3s 106us/sample - loss: 0.1737 - val_loss: 0.1729\n",
      "Epoch 72/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26800/26800 [==============================] - 2s 73us/sample - loss: 0.1736 - val_loss: 0.1736\n",
      "Epoch 73/300\n",
      "26800/26800 [==============================] - 3s 130us/sample - loss: 0.1735 - val_loss: 0.1717\n",
      "Epoch 74/300\n",
      "26800/26800 [==============================] - 3s 127us/sample - loss: 0.1733 - val_loss: 0.1745\n",
      "Epoch 75/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1738 - val_loss: 0.1752\n",
      "Epoch 76/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1732 - val_loss: 0.1718\n",
      "Epoch 77/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1735 - val_loss: 0.1789\n",
      "Epoch 78/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1734 - val_loss: 0.1740\n",
      "Epoch 79/300\n",
      "26800/26800 [==============================] - 3s 127us/sample - loss: 0.1738 - val_loss: 0.1721\n",
      "Epoch 80/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1729 - val_loss: 0.1722\n",
      "Epoch 81/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1743 - val_loss: 0.1714\n",
      "Epoch 82/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1735 - val_loss: 0.1729\n",
      "Epoch 83/300\n",
      "26800/26800 [==============================] - 1s 51us/sample - loss: 0.1735 - val_loss: 0.1715\n",
      "Epoch 84/300\n",
      "26800/26800 [==============================] - 1s 50us/sample - loss: 0.1733 - val_loss: 0.1721\n",
      "Epoch 85/300\n",
      "26800/26800 [==============================] - 1s 49us/sample - loss: 0.1733 - val_loss: 0.1728\n",
      "Epoch 86/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1736 - val_loss: 0.1726\n",
      "Epoch 87/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1735 - val_loss: 0.1711\n",
      "Epoch 88/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1734 - val_loss: 0.1726\n",
      "Epoch 89/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1732 - val_loss: 0.1712\n",
      "Epoch 90/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1733 - val_loss: 0.1751\n",
      "Epoch 91/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1732 - val_loss: 0.1726\n",
      "Epoch 92/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1733 - val_loss: 0.1718\n",
      "Epoch 93/300\n",
      "26800/26800 [==============================] - 2s 58us/sample - loss: 0.1730 - val_loss: 0.1716\n",
      "Epoch 94/300\n",
      "26800/26800 [==============================] - 3s 123us/sample - loss: 0.1732 - val_loss: 0.1724\n",
      "Epoch 95/300\n",
      "26800/26800 [==============================] - 3s 97us/sample - loss: 0.1732 - val_loss: 0.1726\n",
      "Epoch 96/300\n",
      "26800/26800 [==============================] - 3s 123us/sample - loss: 0.1736 - val_loss: 0.1725\n",
      "Epoch 97/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1729 - val_loss: 0.1724\n",
      "Epoch 98/300\n",
      "26800/26800 [==============================] - 2s 73us/sample - loss: 0.1732 - val_loss: 0.1729\n",
      "Epoch 99/300\n",
      "26800/26800 [==============================] - 2s 73us/sample - loss: 0.1730 - val_loss: 0.1721\n",
      "Epoch 100/300\n",
      "26800/26800 [==============================] - 2s 73us/sample - loss: 0.1734 - val_loss: 0.1711\n",
      "Epoch 101/300\n",
      "26800/26800 [==============================] - 2s 82us/sample - loss: 0.1727 - val_loss: 0.1715\n",
      "Epoch 102/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1726 - val_loss: 0.1787\n",
      "Epoch 103/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1728 - val_loss: 0.1712\n",
      "Epoch 104/300\n",
      "26800/26800 [==============================] - 3s 112us/sample - loss: 0.1730 - val_loss: 0.1709\n",
      "Epoch 105/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1734 - val_loss: 0.1713\n",
      "Epoch 106/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1727 - val_loss: 0.1717\n",
      "Epoch 107/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1727 - val_loss: 0.1714\n",
      "Epoch 108/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1725 - val_loss: 0.1713\n",
      "Epoch 109/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1729 - val_loss: 0.1715\n",
      "Epoch 110/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1729 - val_loss: 0.1711\n",
      "Epoch 111/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1726 - val_loss: 0.1716\n",
      "Epoch 112/300\n",
      "26800/26800 [==============================] - 3s 117us/sample - loss: 0.1728 - val_loss: 0.1718\n",
      "Epoch 113/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1727 - val_loss: 0.1716\n",
      "Epoch 114/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1734 - val_loss: 0.1709\n",
      "Epoch 115/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1727 - val_loss: 0.1716\n",
      "Epoch 116/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1728 - val_loss: 0.1711\n",
      "Epoch 117/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1726 - val_loss: 0.1714\n",
      "Epoch 118/300\n",
      "26800/26800 [==============================] - 2s 83us/sample - loss: 0.1724 - val_loss: 0.1724\n",
      "Epoch 119/300\n",
      "26800/26800 [==============================] - 2s 92us/sample - loss: 0.1725 - val_loss: 0.1723\n",
      "Epoch 120/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1728 - val_loss: 0.1724\n",
      "Epoch 121/300\n",
      "26800/26800 [==============================] - 3s 124us/sample - loss: 0.1726 - val_loss: 0.1708\n",
      "Epoch 122/300\n",
      "26800/26800 [==============================] - 3s 129us/sample - loss: 0.1725 - val_loss: 0.1705\n",
      "Epoch 123/300\n",
      "26800/26800 [==============================] - 3s 101us/sample - loss: 0.1723 - val_loss: 0.1720\n",
      "Epoch 124/300\n",
      "26800/26800 [==============================] - 1s 48us/sample - loss: 0.1722 - val_loss: 0.1717\n",
      "Epoch 125/300\n",
      "26800/26800 [==============================] - 1s 48us/sample - loss: 0.1727 - val_loss: 0.1714\n",
      "Epoch 126/300\n",
      "26800/26800 [==============================] - 1s 55us/sample - loss: 0.1725 - val_loss: 0.1722\n",
      "Epoch 127/300\n",
      "26800/26800 [==============================] - 4s 136us/sample - loss: 0.1727 - val_loss: 0.1716\n",
      "Epoch 128/300\n",
      "26800/26800 [==============================] - 4s 143us/sample - loss: 0.1723 - val_loss: 0.1710\n",
      "Epoch 129/300\n",
      "26800/26800 [==============================] - 4s 143us/sample - loss: 0.1723 - val_loss: 0.1711\n",
      "Epoch 130/300\n",
      "26800/26800 [==============================] - 4s 139us/sample - loss: 0.1719 - val_loss: 0.1715\n",
      "Epoch 131/300\n",
      "26800/26800 [==============================] - 2s 87us/sample - loss: 0.1726 - val_loss: 0.1714\n",
      "Epoch 132/300\n",
      "26800/26800 [==============================] - 3s 110us/sample - loss: 0.1720 - val_loss: 0.1715\n",
      "Epoch 133/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1723 - val_loss: 0.1737\n",
      "Epoch 134/300\n",
      "26800/26800 [==============================] - 1s 51us/sample - loss: 0.1725 - val_loss: 0.1727\n",
      "Epoch 135/300\n",
      "26800/26800 [==============================] - 1s 50us/sample - loss: 0.1724 - val_loss: 0.1727\n",
      "Epoch 136/300\n",
      "26800/26800 [==============================] - 1s 49us/sample - loss: 0.1724 - val_loss: 0.1714\n",
      "Epoch 137/300\n",
      "26800/26800 [==============================] - 1s 49us/sample - loss: 0.1724 - val_loss: 0.1743\n",
      "Epoch 138/300\n",
      "26800/26800 [==============================] - 1s 49us/sample - loss: 0.1723 - val_loss: 0.1713\n",
      "Epoch 139/300\n",
      "26800/26800 [==============================] - 1s 49us/sample - loss: 0.1720 - val_loss: 0.1711\n",
      "Epoch 140/300\n",
      "26800/26800 [==============================] - 4s 136us/sample - loss: 0.1724 - val_loss: 0.1708\n",
      "Epoch 141/300\n",
      "26800/26800 [==============================] - 4s 134us/sample - loss: 0.1725 - val_loss: 0.1710\n",
      "Epoch 142/300\n",
      "26800/26800 [==============================] - 3s 94us/sample - loss: 0.1724 - val_loss: 0.1709\n",
      "Epoch 143/300\n",
      "26800/26800 [==============================] - 3s 115us/sample - loss: 0.1724 - val_loss: 0.1709\n",
      "Epoch 144/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1720 - val_loss: 0.1716\n",
      "Epoch 145/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1721 - val_loss: 0.1715\n",
      "Epoch 146/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26800/26800 [==============================] - 3s 131us/sample - loss: 0.1721 - val_loss: 0.1716\n",
      "Epoch 147/300\n",
      "26800/26800 [==============================] - 3s 127us/sample - loss: 0.1720 - val_loss: 0.1729\n",
      "Epoch 148/300\n",
      "26800/26800 [==============================] - 3s 98us/sample - loss: 0.1721 - val_loss: 0.1707\n",
      "Epoch 149/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1719 - val_loss: 0.1737\n",
      "Epoch 150/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1721 - val_loss: 0.1723\n",
      "Epoch 151/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1721 - val_loss: 0.1706\n",
      "Epoch 152/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1719 - val_loss: 0.1733\n",
      "Epoch 153/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1722 - val_loss: 0.1711\n",
      "Epoch 154/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1718 - val_loss: 0.1721\n",
      "Epoch 155/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1718 - val_loss: 0.1709\n",
      "Epoch 156/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1723 - val_loss: 0.1709\n",
      "Epoch 157/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1720 - val_loss: 0.1708\n",
      "Epoch 158/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1721 - val_loss: 0.1718\n",
      "Epoch 159/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1720 - val_loss: 0.1713\n",
      "Epoch 160/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1718 - val_loss: 0.1722\n",
      "Epoch 161/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1718 - val_loss: 0.1708\n",
      "Epoch 162/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1719 - val_loss: 0.1728\n",
      "Epoch 163/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1718 - val_loss: 0.1706\n",
      "Epoch 164/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1719 - val_loss: 0.1714\n",
      "Epoch 165/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1716 - val_loss: 0.1706\n",
      "Epoch 166/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1716 - val_loss: 0.1711\n",
      "Epoch 167/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1717 - val_loss: 0.1716\n",
      "Epoch 168/300\n",
      "26800/26800 [==============================] - 3s 116us/sample - loss: 0.1718 - val_loss: 0.1717\n",
      "Epoch 169/300\n",
      "26800/26800 [==============================] - 2s 65us/sample - loss: 0.1718 - val_loss: 0.1725\n",
      "Epoch 170/300\n",
      "26800/26800 [==============================] - 1s 52us/sample - loss: 0.1716 - val_loss: 0.1713\n",
      "Epoch 171/300\n",
      "26800/26800 [==============================] - 1s 52us/sample - loss: 0.1720 - val_loss: 0.1726\n",
      "Epoch 172/300\n",
      "26800/26800 [==============================] - 1s 50us/sample - loss: 0.1715 - val_loss: 0.1713\n",
      "Epoch 173/300\n",
      "26800/26800 [==============================] - 2s 67us/sample - loss: 0.1715 - val_loss: 0.1705\n",
      "Epoch 174/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1714 - val_loss: 0.1714\n",
      "Epoch 175/300\n",
      "26800/26800 [==============================] - 3s 97us/sample - loss: 0.1716 - val_loss: 0.1718\n",
      "Epoch 176/300\n",
      "26800/26800 [==============================] - 2s 83us/sample - loss: 0.1715 - val_loss: 0.1710\n",
      "Epoch 177/300\n",
      "26800/26800 [==============================] - 2s 82us/sample - loss: 0.1715 - val_loss: 0.1705\n",
      "Epoch 178/300\n",
      "26800/26800 [==============================] - 2s 92us/sample - loss: 0.1718 - val_loss: 0.1703\n",
      "Epoch 179/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1714 - val_loss: 0.1712\n",
      "Epoch 180/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1717 - val_loss: 0.1704\n",
      "Epoch 181/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1715 - val_loss: 0.1711\n",
      "Epoch 182/300\n",
      "26800/26800 [==============================] - 3s 127us/sample - loss: 0.1714 - val_loss: 0.1698\n",
      "Epoch 183/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1712 - val_loss: 0.1713\n",
      "Epoch 184/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1714 - val_loss: 0.1705\n",
      "Epoch 185/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1714 - val_loss: 0.1703\n",
      "Epoch 186/300\n",
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1719 - val_loss: 0.1715\n",
      "Epoch 187/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1714 - val_loss: 0.1708\n",
      "Epoch 188/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1715 - val_loss: 0.1711\n",
      "Epoch 189/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1709 - val_loss: 0.1710\n",
      "Epoch 190/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1714 - val_loss: 0.1708\n",
      "Epoch 191/300\n",
      "26800/26800 [==============================] - 3s 114us/sample - loss: 0.1711 - val_loss: 0.1705\n",
      "Epoch 192/300\n",
      "26800/26800 [==============================] - 3s 111us/sample - loss: 0.1711 - val_loss: 0.1703\n",
      "Epoch 193/300\n",
      "26800/26800 [==============================] - 3s 103us/sample - loss: 0.1710 - val_loss: 0.1735\n",
      "Epoch 194/300\n",
      "26800/26800 [==============================] - 2s 84us/sample - loss: 0.1712 - val_loss: 0.1709\n",
      "Epoch 195/300\n",
      "26800/26800 [==============================] - 2s 67us/sample - loss: 0.1708 - val_loss: 0.1725\n",
      "Epoch 196/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1712 - val_loss: 0.1705\n",
      "Epoch 197/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1711 - val_loss: 0.1707\n",
      "Epoch 198/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1710 - val_loss: 0.1703\n",
      "Epoch 199/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1706 - val_loss: 0.1731\n",
      "Epoch 200/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1714 - val_loss: 0.1718\n",
      "Epoch 201/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1709 - val_loss: 0.1714\n",
      "Epoch 202/300\n",
      "26800/26800 [==============================] - 3s 122us/sample - loss: 0.1710 - val_loss: 0.1711\n",
      "Epoch 203/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1709 - val_loss: 0.1708\n",
      "Epoch 204/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1714 - val_loss: 0.1699\n",
      "Epoch 205/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1706 - val_loss: 0.1702\n",
      "Epoch 206/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1705 - val_loss: 0.1724\n",
      "Epoch 207/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1710 - val_loss: 0.1711\n",
      "Epoch 208/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1710 - val_loss: 0.1700\n",
      "Epoch 209/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1708 - val_loss: 0.1700\n",
      "Epoch 210/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1709 - val_loss: 0.1708\n",
      "Epoch 211/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1705 - val_loss: 0.1714\n",
      "Epoch 212/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1709 - val_loss: 0.1710\n",
      "Epoch 213/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1706 - val_loss: 0.1700\n",
      "Epoch 214/300\n",
      "26800/26800 [==============================] - 2s 89us/sample - loss: 0.1707 - val_loss: 0.1717\n",
      "Epoch 215/300\n",
      "26800/26800 [==============================] - 2s 69us/sample - loss: 0.1705 - val_loss: 0.1697\n",
      "Epoch 216/300\n",
      "26800/26800 [==============================] - 3s 129us/sample - loss: 0.1705 - val_loss: 0.1699\n",
      "Epoch 217/300\n",
      "26800/26800 [==============================] - 3s 123us/sample - loss: 0.1703 - val_loss: 0.1717\n",
      "Epoch 218/300\n",
      "26800/26800 [==============================] - 3s 122us/sample - loss: 0.1705 - val_loss: 0.1697\n",
      "Epoch 219/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1707 - val_loss: 0.1702\n",
      "Epoch 220/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26800/26800 [==============================] - 3s 126us/sample - loss: 0.1708 - val_loss: 0.1708\n",
      "Epoch 221/300\n",
      "26800/26800 [==============================] - 2s 80us/sample - loss: 0.1701 - val_loss: 0.1717\n",
      "Epoch 222/300\n",
      "26800/26800 [==============================] - 2s 60us/sample - loss: 0.1703 - val_loss: 0.1694\n",
      "Epoch 223/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1708 - val_loss: 0.1708\n",
      "Epoch 224/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1704 - val_loss: 0.1718\n",
      "Epoch 225/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1703 - val_loss: 0.1733\n",
      "Epoch 226/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1702 - val_loss: 0.1704\n",
      "Epoch 227/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1703 - val_loss: 0.1703\n",
      "Epoch 228/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1700 - val_loss: 0.1698\n",
      "Epoch 229/300\n",
      "26800/26800 [==============================] - 2s 79us/sample - loss: 0.1703 - val_loss: 0.1699\n",
      "Epoch 230/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1697 - val_loss: 0.1713\n",
      "Epoch 231/300\n",
      "26800/26800 [==============================] - 3s 108us/sample - loss: 0.1700 - val_loss: 0.1746\n",
      "Epoch 232/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1704 - val_loss: 0.1707\n",
      "Epoch 233/300\n",
      "26800/26800 [==============================] - 3s 122us/sample - loss: 0.1702 - val_loss: 0.1703\n",
      "Epoch 234/300\n",
      "26800/26800 [==============================] - 3s 120us/sample - loss: 0.1701 - val_loss: 0.1704\n",
      "Epoch 235/300\n",
      "26800/26800 [==============================] - 3s 121us/sample - loss: 0.1700 - val_loss: 0.1698\n",
      "Epoch 236/300\n",
      "26800/26800 [==============================] - 2s 86us/sample - loss: 0.1706 - val_loss: 0.1711ss: 0.17\n",
      "Epoch 237/300\n",
      "26800/26800 [==============================] - 2s 62us/sample - loss: 0.1700 - val_loss: 0.1700\n",
      "Epoch 238/300\n",
      "26800/26800 [==============================] - 2s 65us/sample - loss: 0.1701 - val_loss: 0.1726\n",
      "Epoch 239/300\n",
      "26800/26800 [==============================] - 2s 78us/sample - loss: 0.1700 - val_loss: 0.1708\n",
      "Epoch 240/300\n",
      "26800/26800 [==============================] - 2s 88us/sample - loss: 0.1700 - val_loss: 0.1696\n",
      "Epoch 241/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1702 - val_loss: 0.1698\n",
      "Epoch 242/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1701 - val_loss: 0.1700\n",
      "Epoch 243/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1702 - val_loss: 0.1697\n",
      "Epoch 244/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1701 - val_loss: 0.1693\n",
      "Epoch 245/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1702 - val_loss: 0.1695\n",
      "Epoch 246/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1697 - val_loss: 0.1701\n",
      "Epoch 247/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1699 - val_loss: 0.1696\n",
      "Epoch 248/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1700 - val_loss: 0.1699\n",
      "Epoch 249/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1698 - val_loss: 0.1698\n",
      "Epoch 250/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1698 - val_loss: 0.1713\n",
      "Epoch 251/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1698 - val_loss: 0.1692\n",
      "Epoch 252/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1695 - val_loss: 0.1700\n",
      "Epoch 253/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1696 - val_loss: 0.1717\n",
      "Epoch 254/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1703 - val_loss: 0.1713\n",
      "Epoch 255/300\n",
      "26800/26800 [==============================] - 3s 117us/sample - loss: 0.1699 - val_loss: 0.1695\n",
      "Epoch 256/300\n",
      "26800/26800 [==============================] - 2s 63us/sample - loss: 0.1695 - val_loss: 0.1731\n",
      "Epoch 257/300\n",
      "26800/26800 [==============================] - 2s 69us/sample - loss: 0.1700 - val_loss: 0.1703\n",
      "Epoch 258/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1698 - val_loss: 0.1703\n",
      "Epoch 259/300\n",
      "26800/26800 [==============================] - 3s 109us/sample - loss: 0.1693 - val_loss: 0.1707\n",
      "Epoch 260/300\n",
      "26800/26800 [==============================] - 1s 51us/sample - loss: 0.1695 - val_loss: 0.1715\n",
      "Epoch 261/300\n",
      "26800/26800 [==============================] - 1s 49us/sample - loss: 0.1694 - val_loss: 0.1739\n",
      "Epoch 262/300\n",
      "26800/26800 [==============================] - 1s 48us/sample - loss: 0.1699 - val_loss: 0.1691\n",
      "Epoch 263/300\n",
      "26800/26800 [==============================] - 1s 48us/sample - loss: 0.1694 - val_loss: 0.1705\n",
      "Epoch 264/300\n",
      "26800/26800 [==============================] - 1s 46us/sample - loss: 0.1698 - val_loss: 0.1709\n",
      "Epoch 265/300\n",
      "26800/26800 [==============================] - 1s 46us/sample - loss: 0.1691 - val_loss: 0.1768\n",
      "Epoch 266/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1695 - val_loss: 0.1752\n",
      "Epoch 267/300\n",
      "26800/26800 [==============================] - 1s 46us/sample - loss: 0.1698 - val_loss: 0.1725\n",
      "Epoch 268/300\n",
      "26800/26800 [==============================] - 1s 46us/sample - loss: 0.1696 - val_loss: 0.1699\n",
      "Epoch 269/300\n",
      "26800/26800 [==============================] - 2s 93us/sample - loss: 0.1698 - val_loss: 0.1702\n",
      "Epoch 270/300\n",
      "26800/26800 [==============================] - 3s 125us/sample - loss: 0.1697 - val_loss: 0.1707\n",
      "Epoch 271/300\n",
      "26800/26800 [==============================] - 1s 47us/sample - loss: 0.1695 - val_loss: 0.1706\n",
      "Epoch 272/300\n",
      "26800/26800 [==============================] - 2s 57us/sample - loss: 0.1698 - val_loss: 0.1705\n",
      "Epoch 273/300\n",
      "26800/26800 [==============================] - 4s 135us/sample - loss: 0.1696 - val_loss: 0.1707\n",
      "Epoch 274/300\n",
      "26800/26800 [==============================] - 3s 128us/sample - loss: 0.1694 - val_loss: 0.1701\n",
      "Epoch 275/300\n",
      "26800/26800 [==============================] - 4s 134us/sample - loss: 0.1697 - val_loss: 0.1738\n",
      "Epoch 276/300\n",
      "26800/26800 [==============================] - 4s 137us/sample - loss: 0.1696 - val_loss: 0.1709\n",
      "Epoch 277/300\n",
      "26800/26800 [==============================] - 4s 137us/sample - loss: 0.1692 - val_loss: 0.1701\n",
      "Epoch 278/300\n",
      "26800/26800 [==============================] - 3s 99us/sample - loss: 0.1695 - val_loss: 0.1721\n",
      "Epoch 279/300\n",
      "26800/26800 [==============================] - 2s 81us/sample - loss: 0.1695 - val_loss: 0.1703\n",
      "Epoch 280/300\n",
      "26800/26800 [==============================] - 2s 92us/sample - loss: 0.1694 - val_loss: 0.1702\n",
      "Epoch 281/300\n",
      "26800/26800 [==============================] - 4s 137us/sample - loss: 0.1691 - val_loss: 0.1702\n",
      "Epoch 282/300\n",
      "26800/26800 [==============================] - 3s 100us/sample - loss: 0.1694 - val_loss: 0.1699\n",
      "Epoch 283/300\n",
      "26800/26800 [==============================] - 3s 103us/sample - loss: 0.1698 - val_loss: 0.1701\n",
      "Epoch 284/300\n",
      "26800/26800 [==============================] - 4s 138us/sample - loss: 0.1693 - val_loss: 0.1703\n",
      "Epoch 285/300\n",
      "26800/26800 [==============================] - 4s 132us/sample - loss: 0.1694 - val_loss: 0.1713\n",
      "Epoch 286/300\n",
      "26800/26800 [==============================] - 4s 137us/sample - loss: 0.1694 - val_loss: 0.1702\n",
      "Epoch 287/300\n",
      "26800/26800 [==============================] - 2s 90us/sample - loss: 0.1694 - val_loss: 0.1704\n",
      "Epoch 288/300\n",
      "26800/26800 [==============================] - 2s 59us/sample - loss: 0.1692 - val_loss: 0.1697\n",
      "Epoch 289/300\n",
      "26800/26800 [==============================] - 3s 94us/sample - loss: 0.1701 - val_loss: 0.1701\n",
      "Epoch 290/300\n",
      "26800/26800 [==============================] - 3s 117us/sample - loss: 0.1691 - val_loss: 0.1697\n",
      "Epoch 291/300\n",
      "26800/26800 [==============================] - 4s 135us/sample - loss: 0.1693 - val_loss: 0.1701\n",
      "Epoch 292/300\n",
      "26800/26800 [==============================] - 4s 134us/sample - loss: 0.1692 - val_loss: 0.1720\n",
      "Epoch 293/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1696 - val_loss: 0.1697\n",
      "Epoch 294/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26800/26800 [==============================] - 3s 122us/sample - loss: 0.1689 - val_loss: 0.1699\n",
      "Epoch 295/300\n",
      "26800/26800 [==============================] - 3s 95us/sample - loss: 0.1694 - val_loss: 0.1703\n",
      "Epoch 296/300\n",
      "26800/26800 [==============================] - 3s 99us/sample - loss: 0.1695 - val_loss: 0.1704\n",
      "Epoch 297/300\n",
      "26800/26800 [==============================] - 3s 119us/sample - loss: 0.1693 - val_loss: 0.1705\n",
      "Epoch 298/300\n",
      "26800/26800 [==============================] - 3s 117us/sample - loss: 0.1688 - val_loss: 0.1712\n",
      "Epoch 299/300\n",
      "26800/26800 [==============================] - 3s 118us/sample - loss: 0.1695 - val_loss: 0.1698\n",
      "Epoch 300/300\n",
      "26800/26800 [==============================] - 2s 89us/sample - loss: 0.1689 - val_loss: 0.1710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16c3d5a1588>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,y=y_train,epochs =300,validation_data =(x_test,y_test),verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "verikaybi = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x16c3d81df48>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "verikaybi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLDUKÇA BAŞARILI BİR SONUÇ OLARAK GÖZÜKÜYOR.ŞİMDİ TAHMİNLERE BAKALIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tahminlerimiz = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#confusion_matrix sınflandırmamızn ne kdar düzgün sonuç verdiğini gösterir\n",
    "#classification_report da genel bir rapor içerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96     12235\n",
      "           1       0.57      0.26      0.36       965\n",
      "\n",
      "    accuracy                           0.93     13200\n",
      "   macro avg       0.76      0.62      0.66     13200\n",
      "weighted avg       0.92      0.93      0.92     13200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,tahminlerimiz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada öellikle precision bakarım ki modelimiz 0 ları %94 1 leri ise %57 doğru tahmin ediyor.\n",
    "0 lar vadeli mevduat geçmeyenleri,\n",
    "1 ise vadeli mevduata geçenleri temsil etmektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12046   189]\n",
      " [  713   252]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,tahminlerimiz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada ise 189 tahmini yanlış bulduğunu, diğerleini dogru bulduğunu görebiliriz.\n",
    "Modelimiz eğer istendiği gibi peformans göstermezse model içerisnde bir çok noktada değişiklikler yapabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
